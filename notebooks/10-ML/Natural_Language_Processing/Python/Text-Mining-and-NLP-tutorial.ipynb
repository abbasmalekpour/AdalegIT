{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stopwords', 'wordnet.zip', 'stopwords.zip', 'gutenberg', 'shakespeare.zip', 'shakespeare', 'wordnet', 'gutenberg.zip', 'state_union.zip', 'state_union']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a_and_c.xml',\n",
       " 'dream.xml',\n",
       " 'hamlet.xml',\n",
       " 'j_caesar.xml',\n",
       " 'macbeth.xml',\n",
       " 'merchant.xml',\n",
       " 'othello.xml',\n",
       " 'r_and_j.xml']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.corpus.gutenberg.fileids()\n",
    "# nltk.download('state_union')\n",
    "# nltk.corpus.state_union.fileids()\n",
    "# nltk.download('shakespeare')\n",
    "nltk.corpus.shakespeare.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/amalekpour/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a document of type string\n",
    "nltk.download('gutenberg')\n",
    "hamlet=nltk.corpus.gutenberg.words(\"shakespeare-macbeth.txt\")\n",
    "hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ The Tragedie of Macbeth by William Shakespeare 1603 ] Actus Primus . Scoena Prima . Thunder and Lightning . Enter three Witches . 1 . When shall we three meet againe ? In Thunder , Lightning , or in Raine ? 2 . When the Hurley - burley ' s done , When the Battaile ' s lost , and wonne 3 . That will be ere the set of Sunne 1 . Where the place ? 2 . Vpon the Heath 3 . There to meet with Macbeth 1 . I come , Gray - Malkin All . "
     ]
    }
   ],
   "source": [
    "for word in hamlet[:100]:\n",
    "    print(word,sep=' ',end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP=\"\"\"Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n",
    "\n",
    "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.   \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_tokens=word_tokenize(NLP)\n",
    "len(NLP_tokens)\n",
    "# NLP_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'natural': 6, 'language': 5, ',': 5, 'and': 4, 'processing': 2, '(': 2, ')': 2, 'of': 2, 'computers': 2, 'in': 2, ...})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the frequence of words\n",
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist()\n",
    "for word in NLP_tokens:\n",
    "    fdist[word.lower()] +=1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('natural', 6), ('language', 5), (',', 5), ('and', 4), ('processing', 2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_top5=fdist.most_common(5)\n",
    "fdist_top5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## number of paragraph \n",
    "from nltk.tokenize import blankline_tokenize\n",
    "NLP_blank=blankline_tokenize(NLP)\n",
    "len(NLP_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the given document into two words, tree words or n-words\n",
    "from nltk.util import bigrams,trigrams,ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "string=\"Given a sound clip of a person or people speaking, determine the textual representation of the speech. This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed AI-complete (see above)\"\n",
    "s_tokens=nltk.word_tokenize(string)\n",
    "s_bigrams=list(nltk.bigrams(s_tokens))\n",
    "# s_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_trigrams=list(nltk.trigrams(s_tokens))\n",
    "# s_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_Ngrams=list(nltk.ngrams(s_tokens,7))\n",
    "# s_Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming \n",
    "\n",
    "Normalized  words into its base form or root forms\n",
    "\n",
    "    PorterStemmer\n",
    "    LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learn'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn:learn\n",
      "learning:learn\n",
      "learner:learner\n",
      "learns:learn\n"
     ]
    }
   ],
   "source": [
    "words=[\"learn\",\"learning\",\"learner\",\"learns\"]\n",
    "for w in words:\n",
    "    print(w+ \":\" +pst.stem(w))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn:learn\n",
      "learning:learn\n",
      "learner:learn\n",
      "learns:learn\n"
     ]
    }
   ],
   "source": [
    "# another way with lancaterstemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "lst=LancasterStemmer()\n",
    "for w in words:\n",
    "    print(w+ \":\" +lst.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization \n",
    "\n",
    "Group together different inflected forms  of words called lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_len=WordNetLemmatizer()\n",
    "word_len.lemmatize('corpora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn:learn\n",
      "learning:learning\n",
      "learner:learner\n",
      "learns:learns\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(w+ \":\" +word_len.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords.words(\"french\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(\"french\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.cor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
